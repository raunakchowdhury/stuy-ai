## Calvin Lee's Talk
- OCR problem (img classification problem)
- Parametrize a function that takes an img and spits out a list of (box, text)
- Training data | two dicts:
  - uses synthText
  - ICDAR (real-world data)
- Convolutional neural network
  - takes an image and takes some weights (convolutional filters)
  - slides those weights over the image and generates a smaller image (really a tensor)
  - `conv(img, weights)`
- Take image and put it through a bunch of convolutions
- Reduce it to a single piece of data that says whether or not it's a cat or dog (if trying to determine whether cat or dog)
- Find theta through gradient descent
- Data augmentation - how to get more data out of limited training data
  - can take 10k images and turn them into 100k images
  - using rotations to modify image, etc.
  - modify aspect ratio, crop image, magnify images, etc.
  - *can you create junk data to throw the model off?*
    - research is being done in the field, but not conventionally used
- Machine learning paradigm
  - Different problems: image problems, language problems
  - Have a function f(x), which is complicated (y is some sent in French, x is in English)
  - Classic way of thinking about it: searching, sliding windows, "hand-crafted features/algorithms"
  - ML people: y = f(x, theta) and a loss function (a judging function that returns how correct a program is)
    - theta is an enormous network
    - x is a sentence
    - map some char `a` to a 256 long vector
    - **feed this into a black box, which gives you an output; take the gradient and then adjust the black box**
    - repeat
  - one of the issues: **overfitting**
    - what if you memorized the training data
    - the black box has hundreds of millions of params, but it generalizes it instead
    - it learns edge detection and language
- VGG 16 -- does the # of convolutions 16 times
  - not a physical object, and a big software process that transfers some #s into other #s
- A ML model:
  - ...o(A3 * o( A2 * o(o(Ax + b)) + b2) + b3)...
  - each A has 2000x1000 weights
  - o --> max(theta,x)
  - This is called a **folding connected network**, but they aren't used much because they guzzle memory
  - can adjust precisions
- Very hands-off -- "do what works"
- Talked about OCR processing using ML
- Can weigh based on positive and negative examples (returned by loss fxn)
- Another way: very confident that it's wrong
- Sometimes too many positive examples -- resolve that by taking 128 of them
- Can partition to test set and training set
  - otherwise the model returns 100% accuracy
- Calvin's current research: how much perturbation (epsilon) that produces the most loss?
  - Adversarial attacks
  - Defense: adversarial training

- Questions:
  - How did you manage to get ML research positions at Jane St, Google, FB, etc.?
  - How easy is it to get a high caliber internship either in the Cambridge Area or at MIT in general, especially as an underclassman?
  - How is MIT's brand name when it comes to cold-emailing people
  - Culture at MIT?
