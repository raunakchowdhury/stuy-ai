## More NNs
- (x1w1 + .... + x784w784 + (bias)) = z
- f(z) = some function that modifies z
- Reason for hidden layers:
- Hidden layers: uses more computational power

## Perceptrons
- Single layer neural network
- Two params: x and y (+ bias) for checking boolean expressions
  - Each wire has a weight
  - biased value has a fixed val of 1, but also has a weight
- w1x1 + w2x2 + w= = z (the output)
  - this becomes ax + by + c = 0, which is like a line
  - if u plot all the booleans (0 being false and 1 being true):
  - true table:
  - | x | y | and |
  | :------------- | :------------- | :------------- |
  | 1 | 1 | 1 |
  | 1 | 0 | 0 |
  | 0 | 1 | 0 |
  | 0 | 0 | 0 |
  - If you graph all the combinations [(0,0), (0,1), (1,0), (1,1)], then you want to create a line such that all things below the line are false, and the one true (1,1) is true and lies above the line
  - The issue is that xor (which has two true cases) cannot be graphed with a line; instead you need a second degree graph, **therefore necessitating a hidden layer**
- f(z) == step function for the porpose of this exercise
  - step fxn: `1 if z>0 else 0`

# Resumed 2019-05-08
- Want to visualize the world in n dimensions
- Regularization: cut down on a lot of the features to a NN because those inputs are not necessary
